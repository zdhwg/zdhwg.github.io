<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on WG的博客</title>
    <link>https://zdhwg.github.io/post/</link>
    <description>Recent content in Posts on WG的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sun, 15 Mar 2020 10:43:29 +0800</lastBuildDate>
    
	<atom:link href="https://zdhwg.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Cs224n笔记 Lecture8</title>
      <link>https://zdhwg.github.io/post/cs224n%E7%AC%94%E8%AE%B0-lecture8/</link>
      <pubDate>Sun, 15 Mar 2020 10:43:29 +0800</pubDate>
      
      <guid>https://zdhwg.github.io/post/cs224n%E7%AC%94%E8%AE%B0-lecture8/</guid>
      <description>lecture8笔记-Machine Translation,Sequence-to-sequence and attention 1.machine tranlation:从一个语言翻译到另一个语言，起源于冷战时期，最初是将俄语翻译成英语。最初是基于规则的，按照词典的对照转换；后来逐渐发展出按统计规则的SMT，但SMT太复杂了，最终神经机器翻译NMT横空出世，机器翻译迎来了新的发展。</description>
    </item>
    
    <item>
      <title>Cs224n笔记 Lecture7</title>
      <link>https://zdhwg.github.io/post/cs224n%E7%AC%94%E8%AE%B0-lecture7/</link>
      <pubDate>Sat, 14 Mar 2020 10:28:07 +0800</pubDate>
      
      <guid>https://zdhwg.github.io/post/cs224n%E7%AC%94%E8%AE%B0-lecture7/</guid>
      <description>lecture7笔记-vanishing gradients and fancy Rnns 1.梯度消失：由于链式法则（chain rule),在对深层网络反向传播梯度时，可能会出现梯度越乘越小的情况，此即为梯度消失。RNN中的梯度消失和一般的深层神经网络的梯度消失概念有所不同，原因在于RNN权重共享，总梯度为各个时间步的梯</description>
    </item>
    
    <item>
      <title>Cs224n笔记-lecture6</title>
      <link>https://zdhwg.github.io/post/cs224n_lecture6%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Wed, 11 Mar 2020 18:54:32 +0800</pubDate>
      
      <guid>https://zdhwg.github.io/post/cs224n_lecture6%E7%AC%94%E8%AE%B0/</guid>
      <description>lecture6 Language Models and Recurrent Nerual Networks 1.language model:就是根据已知序列推测下一个单词（或序列）的问题。输入法、浏览器搜索都有语言模型（根据你输入的单词推测下一个单词或短语）。 2.n-grams:最经典的language model是n-gram,它是基于多个单词在一起使用的统计特性，推测下一个单词时运用了条</description>
    </item>
    
    <item>
      <title>Cs224n笔记 Lecture5</title>
      <link>https://zdhwg.github.io/post/cs224n%E7%AC%94%E8%AE%B0-lecture5/</link>
      <pubDate>Tue, 10 Mar 2020 21:42:13 +0800</pubDate>
      
      <guid>https://zdhwg.github.io/post/cs224n%E7%AC%94%E8%AE%B0-lecture5/</guid>
      <description>lecture5-Dependency Parsing(笔记) 1.为了正确解释理解语言，首先需要理解句子的结构。例如下面的例子： San Jose cops kill man with knife 这句话可能会有歧义，一种理解是：警察用刀杀了那个男子。 这种翻译的理解是： cops 是 kill 的 subject (subject 指 主语) man 是 kill的 object (object 指 宾语) knife 是 kill 的 modifier (modifier 指 修饰符) 另一种理解是：警察杀了那个有刀的男子</description>
    </item>
    
    <item>
      <title>Cs224n笔记 Lecture4</title>
      <link>https://zdhwg.github.io/post/cs224n%E7%AC%94%E8%AE%B0-lecture4/</link>
      <pubDate>Wed, 04 Mar 2020 16:49:29 +0800</pubDate>
      
      <guid>https://zdhwg.github.io/post/cs224n%E7%AC%94%E8%AE%B0-lecture4/</guid>
      <description>lecture4:Backpropagation and computation graphs(笔记) 1.问题：在使用预训练词向量时，比如进行情感分类，在&amp;quot;fine tune&amp;quot;时训练集的词向量会move around，而测试 集的词向量未发生变化，由此在测试集上测试时可能会出现偏差。 答：首先不能抛弃预训练的词向量，在面对规模较小的数据集时，</description>
    </item>
    
    <item>
      <title>理解Glove模型</title>
      <link>https://zdhwg.github.io/post/%E7%90%86%E8%A7%A3Glove%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Mon, 17 Feb 2020 19:49:09 +0800</pubDate>
      
      <guid>https://zdhwg.github.io/post/%E7%90%86%E8%A7%A3Glove%E6%A8%A1%E5%9E%8B/</guid>
      <description>Glove是词的向量化表示方法之一，常用的词的向量化表示方法有：word2vec、glove、ELMo、 BERT。首先先介绍一下除Glove以外的其他三种方法。 word2vec word2vec是2013年提出的方法， 它的核心思想是通过词的上下文得到词的向量化表示，有两种方法： CBOW（通过附近</description>
    </item>
    
    <item>
      <title>Keras中embedding层的作用</title>
      <link>https://zdhwg.github.io/post/keras%E4%B8%ADembedding%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8/</link>
      <pubDate>Sat, 15 Feb 2020 15:23:32 +0800</pubDate>
      
      <guid>https://zdhwg.github.io/post/keras%E4%B8%ADembedding%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8/</guid>
      <description>一、深度学习中embedding层的作用是什么？在做NLP相关工作时经常会与embedding层打交道，在 查阅了有关资料后，将其作用和用法记录如下。 首先，使用embedding主要有两大原因： 1.使用One-hot 方法编码的向量会很高维也很稀疏。假设我们在做自然语言处理（NLP）</description>
    </item>
    
    <item>
      <title>Hugo博客如何新建及上传文章</title>
      <link>https://zdhwg.github.io/post/hugo%E5%8D%9A%E5%AE%A2%E5%A6%82%E4%BD%95%E6%96%B0%E5%BB%BA%E5%8F%8A%E4%B8%8A%E4%BC%A0%E6%96%87%E7%AB%A0/</link>
      <pubDate>Thu, 13 Feb 2020 17:48:26 +0800</pubDate>
      
      <guid>https://zdhwg.github.io/post/hugo%E5%8D%9A%E5%AE%A2%E5%A6%82%E4%BD%95%E6%96%B0%E5%BB%BA%E5%8F%8A%E4%B8%8A%E4%BC%A0%E6%96%87%E7%AB%A0/</guid>
      <description>​ 在hugo博客每次要创建新的博客时，可以在命令行cd到myblog的根目录（或者直接在content/post文件夹下新建一个markdown文件），然后运行以下命令： hugo new post/要创建的文件名字.md ​ 在写完文章后，要上传到静态网站上，首先要在站点跟目录（myblog)上执</description>
    </item>
    
  </channel>
</rss>